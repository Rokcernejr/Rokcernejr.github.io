import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import yfinance as yf
import xgboost as xgb
from scipy.stats import norm
from scipy.optimize import minimize
from sklearn.model_selection import train_test_split
import optuna
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
import datetime
import robin_stocks.robinhood as rh
import time
from datetime import datetime, timedelta

# Set up sentiment analysis pipeline
sentiment_analyzer = pipeline('sentiment-analysis')

# Function to scrape Bloomberg News and perform sentiment analysis
def scrape_bloomberg_news():
    url = 'https://www.bloomberg.com'
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    articles = []
    for item in soup.find_all('article'):
        headline = item.find('h3')
        if headline:
            headline_text = headline.get_text(strip=True)
            link = headline.find('a')['href']
            if link.startswith('/'):
                link = url + link
            articles.append((headline_text, link))
    
    return articles

# Function to perform sentiment analysis on headlines
def analyze_sentiments(headlines):
    sentiments = []
    for headline in headlines:
        sentiment = sentiment_analyzer(headline)
        sentiments.append(sentiment[0]['score'] if sentiment[0]['label'] == 'POSITIVE' else -sentiment[0]['score'])
    return np.mean(sentiments)  # Return the average sentiment score

# Function to download financial data from yfinance
def download_data(ticker, start_date, end_date):
    data = yf.download(ticker, start=start_date, end=end_date)
    data['Return'] = data['Adj Close'].pct_change().fillna(0)
    return data

# Function to create features and labels
def create_features_labels(data, seq_length=50):
    X, y = [], []
    for i in range(len(data) - seq_length):
        features = np.column_stack((data['Return'].values[i:i+seq_length],
                                    data['Sentiment'].values[i:i+seq_length]))
        X.append(features)
        y.append(1 if data['Return'].values[i+seq_length] > 0 else 0)
    X, y = np.array(X), np.array(y)
    X = X.reshape(X.shape[0], X.shape[1], 2)  # Add feature dimension
    return X, y

# Define the 1D Convolutional Neural Network
class Conv1DNN(nn.Module):
    def __init__(self, n_filters1, n_filters2, kernel_size1, kernel_size2, fc_units):
        super(Conv1DNN, self).__init__()
        self.conv1 = nn.Conv1d(2, n_filters1, kernel_size=kernel_size1, padding=kernel_size1//2)
        self.conv2 = nn.Conv1d(n_filters1, n_filters2, kernel_size=kernel_size2, padding=kernel_size2//2)
        self.pool = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(n_filters2 * (50 // 4), fc_units)
        self.fc2 = nn.Linear(fc_units, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = x.permute(0, 2, 1)  # Change shape to (batch, channels, seq_length)
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # Flatten
        x = torch.relu(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x

# Training function
def train(model, dataloader, criterion, optimizer, epochs):
    model.train()
    for epoch in range(epochs):
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

# Evaluation function
def evaluate_model(model, X, y):
    model.eval()
    with torch.no_grad():
        y_pred = model(X).numpy().flatten()
        y_true = y.numpy().flatten()
        fpr, tpr, _ = roc_curve(y_true, y_pred)
        roc_auc = auc(fpr, tpr)
    return roc_auc

# Optuna objective function
def objective(trial):
    n_filters1 = trial.suggest_int('n_filters1', 16, 64)
    n_filters2 = trial.suggest_int('n_filters2', 16, 64)
    kernel_size1 = trial.suggest_int('kernel_size1', 3, 7)
    kernel_size2 = trial.suggest_int('kernel_size2', 3, 7)
    fc_units = trial.suggest_int('fc_units', 32, 128)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)
    epochs = trial.suggest_int('epochs', 10, 50)
    
    model = Conv1DNN(n_filters1, n_filters2, kernel_size1, kernel_size2, fc_units)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    train(model, dataloader, criterion, optimizer, epochs)
    roc_auc = evaluate_model(model, X, y)
    return roc_auc

# Train the best model
def train_best_model(best_params):
    model = Conv1DNN(
        n_filters1=best_params['n_filters1'],
        n_filters2=best_params['n_filters2'],
        kernel_size1=best_params['kernel_size1'],
        kernel_size2=best_params['kernel_size2'],
        fc_units=best_params['fc_units']
    )
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])

    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    train(model, dataloader, criterion, optimizer, epochs=best_params['epochs'])
    roc_auc = evaluate_model(model, X, y)
    return model, roc_auc

# Robinhood login
rh.login('rokc3367', 'Roxyisajackass1')

# Function to get the latest data and make predictions
def get_latest_data_and_predict(model, ticker, seq_length=50):
    end_date = datetime.today().strftime('%Y-%m-%d')
    start_date = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')
    data = download_data(ticker, start_date, end_date)

    news_articles = scrape_bloomberg_news()
    data['Date'] = data.index
    data['Sentiment'] = data['Date'].apply(lambda date: analyze_sentiments(
        [headline for headline, link in news_articles if date in link]))

    data['Sentiment'].fillna(data['Sentiment'].mean(), inplace=True)
    
    X, _ = create_features_labels(data)
    X = torch.tensor(X, dtype=torch.float32)
    
    model.eval()
    with torch.no_grad():
        y_pred = model(X).numpy().flatten()
    
    return y_pred[-1]  # Return the latest prediction

# Function to rebalance portfolio
def rebalance_portfolio(model, ticker, threshold=0.5):
    prediction = get_latest_data_and_predict(model, ticker)
    current_holdings = rh.account.build_holdings()
    
    if prediction > threshold:
        # Buy logic
        if ticker not in current_holdings:
            rh.orders.order_buy_market(ticker, 1)
            print(f'Bought 1 share of {ticker}')
    else:
        # Sell logic
        if ticker in current_holdings:
            rh.orders.order_sell_market(ticker, 1)
            print(f'Sold 1 share of {ticker}')

# Schedule daily rebalancing
while True:
    # Train models
    ticker = 'AAPL'
    start_date = '2018-01-01'
    end_date = '2023-01-01'
    data = download_data(ticker, start_date, end_date)

    news_articles = scrape_bloomberg_news()
    data['Date'] = data.index
    data['Sentiment'] = data['Date'].apply(lambda date: analyze_sentiments(
        [headline for headline, link in news_articles if date in link]))

    data['Sentiment'].fillna(data['Sentiment'].mean(), inplace=True)

    X, y = create_features_labels(data)
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
best_params = study.best_params

model, roc_auc = train_best_model(best_params)
print('ROC AUC of the best model:', roc_auc)

# Rebalance portfolio
rebalance_portfolio(model, ticker)

# Wait for 24 hours before rebalancing again
time.sleep(86400)

### Important Notes:
- **Sensitive Information**: Never hard-code your login credentials in the script. Use environment variables or secure methods to handle them.
- **Robinhood API Limits**: Robinhood imposes rate limits and restrictions on automated trading. Ensure compliance with their terms of service.
- **Model Training Frequency**: Training the model every day might not be necessary. Depending on your strategy, you could update the model less frequently to save computational resources.
- **Error Handling**: Add error handling to manage network issues, API failures, or any unexpected situations.